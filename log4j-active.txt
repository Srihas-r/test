21/04/12 15:14:07 INFO StaticConf$: DB_HOME: /databricks
21/04/12 15:14:09 INFO DriverDaemon$: Current JVM Version 1.8.0_275
21/04/12 15:14:09 INFO DriverDaemon$: ========== driver starting up ==========
21/04/12 15:14:09 INFO DriverDaemon$: Java: Azul Systems, Inc. 1.8.0_275
21/04/12 15:14:09 INFO DriverDaemon$: OS: Linux/amd64 5.4.0-1039-aws
21/04/12 15:14:09 INFO DriverDaemon$: CWD: /databricks/driver
21/04/12 15:14:09 INFO DriverDaemon$: Mem: Max: 19.5G loaded GCs: PS Scavenge, PS MarkSweep
21/04/12 15:14:09 INFO DriverDaemon$: Logging multibyte characters: âœ“
21/04/12 15:14:09 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
21/04/12 15:14:09 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
21/04/12 15:14:09 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
21/04/12 15:14:09 INFO DriverDaemon$: == Modules:
21/04/12 15:14:10 INFO DriverDaemon$: Starting prometheus metrics log export timer
21/04/12 15:14:10 INFO DriverDaemon$: Universe Git Hash: a72df6176b3ee86c30d1cdebcdc7576f4f10178e
21/04/12 15:14:10 INFO DriverDaemon$: Spark Git Hash: 4bcdca7fa46a870fed896724881c8dba1acc6987
21/04/12 15:14:10 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.,false,false,List())
21/04/12 15:14:10 INFO DatabricksILoop$: Creating throwaway interpreter
21/04/12 15:14:10 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
21/04/12 15:14:10 INFO MetastoreMonitor$: Internal internal metastore configured (config=DbMetastoreConfig{host=cust-success-recovered-from-snapshot.caj77bnxuhme.us-west-2.rds.amazonaws.com, port=3306, dbName=organization0, user=zvIXSyBKCfBVEEWo})
21/04/12 15:14:11 INFO HikariDataSource: metastore-monitor - Starting...
21/04/12 15:14:11 INFO HikariDataSource: metastore-monitor - Start completed.
21/04/12 15:14:11 INFO DriverCorral: Creating the driver context
21/04/12 15:14:11 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-2519914222661258662-7e975dbb-8e97-4f78-9f79-5584921b7de3
21/04/12 15:14:11 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
21/04/12 15:14:11 INFO HikariDataSource: metastore-monitor - Shutdown completed.
21/04/12 15:14:11 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 929 milliseconds)
21/04/12 15:14:11 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
21/04/12 15:14:11 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
21/04/12 15:14:11 INFO SparkContext: Running Spark version 3.0.1
21/04/12 15:14:12 INFO ResourceUtils: ==============================================================
21/04/12 15:14:12 INFO ResourceUtils: Resources for spark.driver:

21/04/12 15:14:12 INFO ResourceUtils: ==============================================================
21/04/12 15:14:12 INFO SparkContext: Submitted application: Databricks Shell
21/04/12 15:14:12 INFO SecurityManager: Changing view acls to: root
21/04/12 15:14:12 INFO SecurityManager: Changing modify acls to: root
21/04/12 15:14:12 INFO SecurityManager: Changing view acls groups to: 
21/04/12 15:14:12 INFO SecurityManager: Changing modify acls groups to: 
21/04/12 15:14:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
21/04/12 15:14:13 INFO Utils: Successfully started service 'sparkDriver' on port 39021.
21/04/12 15:14:13 INFO SparkEnv: Registering MapOutputTracker
21/04/12 15:14:13 INFO SparkEnv: Registering BlockManagerMaster
21/04/12 15:14:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/04/12 15:14:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/04/12 15:14:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/04/12 15:14:13 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-adee2879-790e-43ee-bc8f-4624438a7e37
21/04/12 15:14:13 INFO MemoryStore: MemoryStore started with capacity 10.7 GiB
21/04/12 15:14:13 INFO SparkEnv: Registering OutputCommitCoordinator
21/04/12 15:14:13 INFO SparkContext: Spark configuration:
eventLog.rolloverIntervalSeconds=3600
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.acl.scim.client=com.databricks.spark.sql.acl.client.DriverToWebappScimClient
spark.databricks.cloudProvider=AWS
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.autoTerminationMinutes=30
spark.databricks.clusterUsageTags.cloudProvider=AWS
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"srihasa.akepati@databricks.com"},{"key":"ClusterName","value":"TestSrihas"},{"key":"ClusterId","value":"0317-064914-rib20"},{"key":"Name","value":"cust-success-worker"}]
spark.databricks.clusterUsageTags.clusterAvailability=SPOT_WITH_FALLBACK
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterEbsVolumeCount=0
spark.databricks.clusterUsageTags.clusterEbsVolumeSize=0
spark.databricks.clusterUsageTags.clusterEbsVolumeType=GENERAL_PURPOSE_SSD
spark.databricks.clusterUsageTags.clusterFirstOnDemand=1
spark.databricks.clusterUsageTags.clusterGeneration=16
spark.databricks.clusterUsageTags.clusterId=0317-064914-rib20
spark.databricks.clusterUsageTags.clusterLastActivityTime=1618238574487
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=2
spark.databricks.clusterUsageTags.clusterName=TestSrihas
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=i3.xlarge
spark.databricks.clusterUsageTags.clusterNumSshKeys=1
spark.databricks.clusterUsageTags.clusterOwnerOrgId=0
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidPricePercent=100
spark.databricks.clusterUsageTags.clusterState=Restarting
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=2
spark.databricks.clusterUsageTags.clusterWorkers=2
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.containerZoneId=us-west-2d
spark.databricks.clusterUsageTags.dataPlaneRegion=us-west-2
spark.databricks.clusterUsageTags.driverContainerId=334ada14626643788b64c2321678d82c
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.205.248.177
spark.databricks.clusterUsageTags.driverInstanceId=i-069a4b60a5a7c6c8a
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.205.238.59
spark.databricks.clusterUsageTags.driverNodeType=i3.xlarge
spark.databricks.clusterUsageTags.driverPublicDns=
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.instanceBootstrapType=ssh
spark.databricks.clusterUsageTags.instanceProfileUsed=false
spark.databricks.clusterUsageTags.instanceWorkerEnvId=default-worker-env
spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType=default
spark.databricks.clusterUsageTags.isIMv2Enabled=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.ngrokNpipEnabled=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=1
spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2=0
spark.databricks.clusterUsageTags.privateLinkEnabled=false
spark.databricks.clusterUsageTags.region=us-west-2
spark.databricks.clusterUsageTags.sparkVersion=7.5.x-scala2.12
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=default-worker-env
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.driverNfs.enabled=true
spark.databricks.driverNfs.pathSuffix=.ephemeral_nfs
spark.databricks.driverNodeTypeId=i3.xlarge
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.repl.enableClassFileCleanup=true
spark.databricks.secret.envVar.keys.toRedact=*********(redacted)
spark.databricks.secret.sparkConf.keys.toRedact=*********(redacted)
spark.databricks.service.dbutils.repl.backend=com.databricks.dbconnect.ReplDBUtils
spark.databricks.service.dbutils.server.backend=com.databricks.dbconnect.SparkServerDBUtils
spark.databricks.session.share=false
spark.databricks.sparkContextId=2519914222661258662
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.tahoe.logStore.gcp.class=com.databricks.tahoe.store.GCPLogStore
spark.databricks.workerNodeTypeId=i3.xlarge
spark.databricks.workspace.matplotlibInline.enabled=true
spark.databricks.workspace.multipleResults.enabled=true
spark.driver.allowMultipleContexts=false
spark.driver.host=10.205.248.177
spark.driver.maxResultSize=4g
spark.driver.port=39021
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=20396m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories=false
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.AbstractFileSystem.gs.impl=shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
spark.hadoop.fs.abfs.impl=shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.gs.impl=shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
spark.hadoop.fs.gs.impl.disable.cache=true
spark.hadoop.fs.s3.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.active.blocks=32
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.block.size.row.check.max=10
spark.hadoop.parquet.block.size.row.check.min=10
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.parquet.page.size.check.estimate=false
spark.hadoop.parquet.page.verify-checksum.enabled=true
spark.hadoop.parquet.page.write-checksum.enabled=true
spark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.sqlGatewayCloseSessionHeaderName=X-Databricks-SqlGateway-CloseSession
spark.hadoop.spark.thriftserver.sqlGatewaySessionIdHeaderName=X-Databricks-SqlGateway-SessionId
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.205.248.177:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-2519914222661258662-7e975dbb-8e97-4f78-9f79-5584921b7de3
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.streaming.stopTimeout=15s
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=42463
spark.worker.cleanup.enabled=false
21/04/12 15:14:13 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/04/12 15:14:13 INFO log: Logging initialized @8343ms to org.eclipse.jetty.util.log.Slf4jLog
21/04/12 15:14:14 INFO Server: jetty-9.4.18.v20190429; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_275-b01
21/04/12 15:14:14 INFO Server: Started @8527ms
21/04/12 15:14:14 INFO AbstractConnector: Started ServerConnector@30bbcf91{HTTP/1.1,[http/1.1]}{10.205.248.177:42463}
21/04/12 15:14:14 INFO Utils: Successfully started service 'SparkUI' on port 42463.
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f1fb828{/jobs,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4017fe2c{/jobs/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@677ce519{/jobs/job,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6b37df8e{/jobs/job/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b351446{/stages,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@a1691c0{/stages/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f995afc{/stages/stage,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7f0a133d{/stages/stage/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@715fa8c5{/stages/pool,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b765e92{/stages/pool/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a70d302{/storage,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@622d7e4{/storage/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@261b9a37{/storage/rdd,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55e1192{/storage/rdd/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68a305eb{/environment,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1310fcb0{/environment/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e8fdd19{/executors,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2199e845{/executors/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@37e0056e{/executors/threadDump,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2fe2965c{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@40943a6{/executors/heapHistogram,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@42679fc2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@100aa331{/static,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@dbddbe3{/,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@44aa2e13{/api,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@46a795de{/jobs/job/kill,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f3928ac{/stages/stage/kill,null,AVAILABLE,@Spark}
21/04/12 15:14:14 INFO SparkUI: Bound SparkUI to 10.205.248.177, and started at http://10.205.248.177:42463
21/04/12 15:14:14 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
21/04/12 15:14:14 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
21/04/12 15:14:14 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
21/04/12 15:14:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.205.248.177:7077...
21/04/12 15:14:14 INFO TransportClientFactory: Successfully created connection to /10.205.248.177:7077 after 93 ms (0 ms spent in bootstraps)
21/04/12 15:14:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210412151415-0000
21/04/12 15:14:15 INFO TaskSchedulerImpl: Task preemption enabled.
21/04/12 15:14:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43471.
21/04/12 15:14:15 INFO NettyBlockTransferService: Server created on 10.205.248.177:43471
21/04/12 15:14:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210412151415-0000/0 on worker-20210412151413-10.205.240.126-32795 (10.205.240.126:32795) with 4 core(s)
21/04/12 15:14:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/04/12 15:14:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20210412151415-0000/0 on hostPort 10.205.240.126:32795 with 4 core(s), 19.9 GiB RAM
21/04/12 15:14:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.205.248.177, 43471, None)
21/04/12 15:14:15 INFO BlockManagerMasterEndpoint: Registering block manager 10.205.248.177:43471 with 10.7 GiB RAM, BlockManagerId(driver, 10.205.248.177, 43471, None)
21/04/12 15:14:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.205.248.177, 43471, None)
21/04/12 15:14:15 INFO BlockManager: external shuffle service port = 4048
21/04/12 15:14:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.205.248.177, 43471, None)
21/04/12 15:14:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@16bd7ae1{/metrics/json,null,AVAILABLE,@Spark}
21/04/12 15:14:15 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
21/04/12 15:14:15 INFO DBCEventLoggingListener: Logging events to eventlogs/2519914222661258662/eventlog
21/04/12 15:14:15 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
21/04/12 15:14:15 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
21/04/12 15:14:15 INFO SparkContext: Loading Spark Service RPC Server
21/04/12 15:14:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210412151415-0000/0 is now RUNNING
21/04/12 15:14:15 WARN SparkServerContext: Could not get dbr version from dbr_versions.txt because: null
21/04/12 15:14:16 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
21/04/12 15:14:16 INFO Server: jetty-9.4.18.v20190429; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_275-b01
21/04/12 15:14:16 INFO AbstractConnector: Started ServerConnector@36c07c75{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}
21/04/12 15:14:16 INFO Server: Started @10680ms
21/04/12 15:14:16 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
21/04/12 15:14:16 INFO DatabricksILoop$: Successfully initialized SparkContext
21/04/12 15:14:17 INFO SharedState: Scheduler stats enabled.
21/04/12 15:14:17 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
21/04/12 15:14:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
21/04/12 15:14:17 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4cae66a8{/SQL,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3962ec84{/SQL/json,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3181d4de{/SQL/execution,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@639c7f9c{/SQL/execution/json,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@ee96866{/static/sql,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210412151415-0000/1 on worker-20210412151417-10.205.237.211-33623 (10.205.237.211:33623) with 4 core(s)
21/04/12 15:14:18 INFO StandaloneSchedulerBackend: Granted executor ID app-20210412151415-0000/1 on hostPort 10.205.237.211:33623 with 4 core(s), 19.9 GiB RAM
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@532dacf5{/storage/iocache,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d31f106{/storage/iocache/json,null,AVAILABLE,@Spark}
21/04/12 15:14:18 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210412151415-0000/1 is now RUNNING
21/04/12 15:14:18 INFO DatabricksILoop$: Finished creating throwaway interpreter
21/04/12 15:14:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 20396, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/04/12 15:14:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.205.240.126:50066) with ID 0
21/04/12 15:14:20 INFO BlockManagerMasterEndpoint: Registering block manager 10.205.240.126:43589 with 10.4 GiB RAM, BlockManagerId(0, 10.205.240.126, 43589, None)
21/04/12 15:14:21 INFO DatabricksMountsStore: Mount store initialization: Attempting to get the list of mounts from metadata manager of DBFS
21/04/12 15:14:21 INFO log: Logging initialized @16271ms to shaded.v9_4.org.eclipse.jetty.util.log.Slf4jLog
21/04/12 15:14:22 INFO TypeUtil: JVM Runtime does not support Modules
21/04/12 15:14:22 INFO DatabricksMountsStore: Mount store initialization: Received a list of 311 mounts accessible from metadata manager of DBFS
21/04/12 15:14:22 INFO DatabricksMountsStore: Updated mounts cache. Changes: List((+,DbfsMountPoint(s3a://db-support/csv/, /mnt/ashwin/encrypted-s3-bucket)), (+,DbfsMountPoint(s3a://test-bucket-rw/, /mnt/ashwin/write)), (+,DbfsMountPoint(s3a://personal-sajesh/, /mnt/sajeshm/gmail.com00)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/training)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/training-dan.zafar@databricks.com)), (+,DbfsMountPoint(s3a://ankur-s3, /mnt/ankurs3)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test4)), (+,DbfsMountPoint(s3a://gauravdbsupport, /mnt/gaurav_aws_bucket)), (+,DbfsMountPoint(s3a://arjun-control-plane-write-test/, /mnt/arjun-control-plane-write-test/)), (+,DbfsMountPoint(s3a://karthikdatabricks01/test, /mnt/ntestk1)), (+,DbfsMountPoint(s3a://neha-db-test-bucket, /mnt/readonly)), (+,DbfsMountPoint(wasbs://johndata.blob.core.windows.net, /mnt/johnblob)), (+,DbfsMountPoint(s3a://ashwin-s3-test, /mnt/ashwin/lib)), (+,DbfsMountPoint(s3a://karthikshivanna, /mnt/ks)), (+,DbfsMountPoint(s3a://cust-success/0/rakesh/jobs.json, /mnt/parija/json)), (+,DbfsMountPoint(s3a://databricks-corp-training/ml-amsterdam/mooc, /mnt/spark-mooc)), (+,DbfsMountPoint(s3a://test-bucket-rw, /mnt/test-bucket-rw)), (+,DbfsMountPoint(s3a://festus-lib-test/Libraries, /mnt/libs)), (+,DbfsMountPoint(s3a://ganesh-s3-bucket, /mnt/ganesh-s3-bucket)), (+,DbfsMountPoint(s3a://chris-s3-us-west2-test, /mnt/christestmount)), (+,DbfsMountPoint(s3a://databricks.mnk-discreet, /mnt/discreet)), (+,DbfsMountPoint(s3a://premcrossacctest/test/test1/credentials.csv, /mnt/test/test1/credentials.csv)), (+,DbfsMountPoint(s3a://arjun-field-eng-s3-cross-account-test, /mnt/arjun-field-eng-s3-cross-account-test)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/date=2018-12-19, /mnt//naresh-test-audit-logs_test)), (+,DbfsMountPoint(s3a://tm-test-s3-1, /mnt/tms31)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur_mnt1)), (+,DbfsMountPoint(s3a://databricks-datasets-oregon/, /databricks-datasets)), (+,DbfsMountPoint(s3a://test-rakesh-subfolder/folder, /mnt/subfolder-rrp)), (+,DbfsMountPoint(s3a://db-wikipedia-readonly-eu, /mnt/wikipedia-readonly-eu)), (+,DbfsMountPoint(s3a://shashitest, /mnt/$shashitest/)), (+,DbfsMountPoint(s3n://abhideltatest, /mnt/MountName)), (+,DbfsMountPoint(s3a://databrics-deployment-root/cust-success/0/user/hive/warehouse/active_item/, /mnt/hive-metastore)), (+,DbfsMountPoint(s3a://arjun-assume-role-test-bucket, /mnt/arjun-assume-role-test-bucket-2)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test0)), (+,DbfsMountPoint(s3a://chris-s3-us-west1, /mnt/chris/mount_test1)), (+,DbfsMountPoint(s3a://personal-sajesh/, /mnt/sajeshm/1/2/3/s3)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/p1)), (+,DbfsMountPoint(s3a://johnlourdu-fieldeng, /mnt/johnlourdu-fieldeng)), (+,DbfsMountPoint(s3a://im-db-test00, /mnt/im_test00)), (+,DbfsMountPoint(s3a://abhideltatest, /mnt/abhideltatestRam)), (+,DbfsMountPoint(s3a://nimmanimma/, /mnt/rohitnimma/)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/, /mnt//naresh-audit-logs_test)), (+,DbfsMountPoint(s3a://db-wikipedia-readonly-use, /mnt/wikipedia-readonly/)), (+,DbfsMountPoint(s3a://ankur-test1/, /mnt/$ankur1/)), (+,DbfsMountPoint(s3a://chris-s3-us-west2, /mnt/chris-s3-us-west2)), (+,DbfsMountPoint(s3a://databricks-corp-training-temp, /mnt/training-temp)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/sajesh-full-no-acl)), (+,DbfsMountPoint(s3a://john-s3, /mnt/john-s3)), (+,DbfsMountPoint(wasbs://opsstoragedev.blob.core.windows.net, /mnt/metadata)), (+,DbfsMountPoint(s3a://databricks-sajesh/list-bucket-new-folder, /mnt/sajesh-new-folder)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovdb)), (+,DbfsMountPoint(s3a://abitest111/com.deere.enterprise.datalake.enhance.elips.elips_header_table%252525401.0.0, /mnt/abitest)), (+,DbfsMountPoint(s3a://abhideltatest, /mnt/abhi_delta_test3)), (+,DbfsMountPoint(s3a://arjun-cust-test-bucket, /mnt/arjun-s3-key-test)), (+,DbfsMountPoint(s3a://autoloadercrossactgobitest/test, /mnt/autoloadermount)), (+,DbfsMountPoint(wasbs://xinzstorage.blob.core.windows.net/, /mnt/mypath)), (+,DbfsMountPoint(s3a://sajeshnewfolder, /mnt/sajeshtestnewfolder)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/sajesh/sajesh-s30)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/pradeepbucket)), (+,DbfsMountPoint(s3a://databrics-deployment-root/cust-success, /mnt/test-cust-success)), (+,DbfsMountPoint(s3a://johnlourdu-fieldeng, /mnt/johnlourdu_files)), (+,DbfsMountPoint(s3a://ashwin-crossaccount/test2, /mnt/ashwin/nike_test/test2)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test10)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/etlp1s-shan-ss)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/p03)), (+,DbfsMountPoint(s3a://shashitest, /mnt//mnt/shashi1)), (+,DbfsMountPoint(s3a://cksparkbucket/, /mnt/hari)), (+,DbfsMountPoint(s3a://premcrossacctest/test2/test3, /mnt/test2/test3)), (+,DbfsMountPoint(s3a://arjun-cust-success-cluster-logs, /mnt/arjun-cust-success-cluster-logs)), (+,DbfsMountPoint(s3a://shashitest, /mnt//mnt/shashitest/)), (+,DbfsMountPoint(s3a://db-wikipedia-readonly-eu, /mnt/wikipedia)), (+,DbfsMountPoint(s3a://karthikdatabricks01, /mnt/newnew)), (+,DbfsMountPoint(s3a://john-redshift, /mnt/john-redshift)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/etlp1s-carlos.morillo@databricks.com-si)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/sajesh-s3/subfolder)), (+,DbfsMountPoint(s3a://rakesh-test-bucket2, /mnt/rakesh-mount-bucket3)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/sajesh/test5)), (+,DbfsMountPoint(s3a://databrics-deployment-root, /mnt/testcust)), (+,DbfsMountPoint(s3a://ckspark/, /mnt/hari_s3/)), (+,DbfsMountPoint(s3a://abhideltatest, /mnt/delta_test)), (+,DbfsMountPoint(s3a://chris-s3-us-west4, /mnt/chris/mount_test4)), (+,DbfsMountPoint(s3a://ankur-test1, /mnt/ankur_s3)), (+,DbfsMountPoint(s3a://gauravdbsupport, /mnt/gaurav_aws_bucket>)), (+,DbfsMountPoint(s3a://arjun-assume-role-test-bucket/, /mnt/arjun-assume-role-test-bucket-Arjun-Test-1)), (+,DbfsMountPoint(s3a://neha-db-test-bucket, /mnt/testneha1)), (+,DbfsMountPoint(s3a://ankur-test1/Training_Prog.xlsx, /mnt/$ankur)), (+,DbfsMountPoint(s3a://piyushpocbucket, /mnt/sourcefolder)), (+,DbfsMountPoint(s3a://ddsharma, /mnt/dds3)), (+,DbfsMountPoint(wasbs://dbtraineastus2.blob.core.windows.net/, /mnt/etlp1a-saritha.shivakumar@databricks.com-si)), (+,DbfsMountPoint(s3a://e2billing, /mnt/e2mount-CMkms)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur_mnt)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/p11)), (+,DbfsMountPoint(s3a://db-fe-sajesh/, /mnt/sajesh_assume_role)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkt)), (+,DbfsMountPoint(s3a://chris-s3-us-west2, /mnt/chris/mount_test2)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovdbb)), (+,DbfsMountPoint(wasbs://opsstoragedev.blob.core.windows.net, /mnt/envision-clean)), (+,DbfsMountPoint(wasbs://dbedueastus2.blob.core.windows.net/, /mnt/training-msft)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/pradeepkumars3mount)), (+,DbfsMountPoint(s3a://atanudestinationbucket, /mnt/atanusource)), (+,DbfsMountPoint(s3a://shashitest, /mnt/$shashitest1/)), (+,DbfsMountPoint(s3a://vpcflowlogbucketnvn/AWSLogs/826763667205/vpcflowlogs, /mnt/vpcflowlogs)), (+,DbfsMountPoint(s3a://karthikshivanna, /mnt/mnt/pradyTest/)), (+,DbfsMountPoint(s3a://arjun-field-eng-s3-cross-account-test-4, /mnt/testingmlflow)), (+,DbfsMountPoint(s3a://cust-success-bucket, /mnt/ganesh/cust-success-bucket)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-tracking)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/p18)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test5)), (+,DbfsMountPoint(adl://festusadls.azuredatalakestore.net, /mnt/training-adl)), (+,DbfsMountPoint(s3a://prem-audit-log, /mnt/premaudit)), (+,DbfsMountPoint(s3a://databricks-yong, /mnt/yong)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/etlp1s-siddharth.panchal@databricks.com-si)), (+,DbfsMountPoint(s3a://arjun-minio-test, /mnt/arjun-minion-test-3)), (+,DbfsMountPoint(s3a://arjun-cust-test-bucket/, /mnt/arjun-cust-test-bucket)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/temp-sajesh-2/)), (+,DbfsMountPoint(s3a://spark-eight-bucket, /mnt/sparkeight)), (+,DbfsMountPoint(s3a://trustingbucket, /mnt/log-delivery)), (+,DbfsMountPoint(s3a://ashwin-crossaccount/test2, /mnt/ashwin/hp_test2)), (+,DbfsMountPoint(s3a://rahulspark, /mnt/rahul)), (+,DbfsMountPoint(s3a://abhideltatest, /mnt/DeltaAuto)), (+,DbfsMountPoint(s3a://karthik, /mnt/karthik01)), (+,DbfsMountPoint(s3a://arjun-cust-test-bucket, /mnt/test-iam-role-mount)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/etlp1s-saritha.shivakumar@databricks.com-si)), (+,DbfsMountPoint(s3a://ankur-s3, /mnt/ankur-s3/ankur-s3_new)), (+,DbfsMountPoint(s3a://tm-test-s3-1, /mnt/takeshi_bucket)), (+,DbfsMountPoint(s3a://databricks-sajesh, /mnt/bro_data)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/temp-sajesh-1/)), (+,DbfsMountPoint(s3a://databrics-deployment-root/ephemeral/cust-success/0, /databricks-results)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test1)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/temp-sajesh/)), (+,DbfsMountPoint(s3a://wguir/Publishers/Vitalsource/activities/temp, /mnt/vs_activitiestemp)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/project, /mnt/chris/project)), (+,DbfsMountPoint(s3n://abhideltatest, /mnt/abhi_delta_test2)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test00001)), (+,DbfsMountPoint(s3a://personal-sajesh/, /mnt/sajeshm/gmail.com0)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/training-michael.artz@databricks.com)), (+,DbfsMountPoint(s3a://databrics-deployment-root/cust-success/0/user/hive/warehouse/internal/, /mnt/hive-metastore/internal)), (+,DbfsMountPoint(s3a://atanusourcebucket, /mnt/mnt)), (+,DbfsMountPoint(s3a://landsat-pds, /mnt/as_landsat)), (+,DbfsMountPoint(s3a://test-bucket, /mnt/parija1)), (+,DbfsMountPoint(s3a://abitest111, /mnt/abi)), (+,DbfsMountPoint(adl://gopiadltest.azuredatalakestore.net/testmount/, /mnt/adl-mount)), (+,DbfsMountPoint(s3a://arjun-minio-test, /mnt/arjun-minion-test-2)), (+,DbfsMountPoint(s3a://ankur-s3/csv, /mnt/ankurs3/csv)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovtestmountzz)), (+,DbfsMountPoint(s3a://databricks-sajesh/, /mnt/sajesh_with_region_1)), (+,DbfsMountPoint(s3a://abizer-testing/com.deere.enterprise.datalake.enhance.elips.elips_header_table%252525401.0.0, /mnt/abi_test)), (+,DbfsMountPoint(s3a://nimmanimma/, /mnt/rohitnimma1/)), (+,DbfsMountPoint(s3a://rakesh-databricks/, /mnt/rajeev)), (+,DbfsMountPoint(s3a://karthikdatabricks01, /mnt/new011new)), (+,DbfsMountPoint(wasbs://shan03.blob.core.windows.net/, /mnt/shanmugavel.chandrakasu@databricks.com-shan)), (+,DbfsMountPoint(s3a://chris-test-mount, /mnt/mount_test/mount_issue)), (+,DbfsMountPoint(s3a://sandeeptestbucket2020, /mnt/sandeeptestbucket2020)), (+,DbfsMountPoint(s3a://ddsharma, /mnt/dd)), (+,DbfsMountPoint(s3a://s3a//pradeepkumarbucket, /mnt/sandeepTest)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-registry)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/date=2018-12-19, /mnt//naresh-cluster-audit-logs_test)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test1)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/, /mnt//sajesh-audit-logs)), (+,DbfsMountPoint(s3a://premcrossacctest/test/test1, /mnt/test/test1)), (+,DbfsMountPoint(s3a://karthikshivanna, /mnt/Nanne)), (+,DbfsMountPoint(s3a://chris-s3-us-west2/, /mnt/chriszhao/test)), (+,DbfsMountPoint(s3a://abitest1111/fo, /mnt/abizer/testbucket1)), (+,DbfsMountPoint(s3a://arjun-field-eng-s3-cross-account-test, /mnt/arjun-field-eng-s3-cross-account-test-config-set)), (+,DbfsMountPoint(s3a://gobi-s3-test/sudoku, /mnt/sudoku)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/date=2018-12-19, /mnt//naresh-cluster-audit-logs_test1)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test8)), (+,DbfsMountPoint(s3a://tm-test-s3-1, /mnt/tm-encrypt-2)), (+,DbfsMountPoint(s3a://festus-lib-test/Libraries, /mnt/logs)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovdub)), (+,DbfsMountPoint(s3a://ashwin-crossaccount/test3, /mnt/ashwin/hp_test1)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur1_mnt)), (+,DbfsMountPoint(s3a://databricks-sajesh, /mnt/sajesh-rakesh2)), (+,DbfsMountPoint(s3a://databricks-corp-training/ml-amsterdam, /mnt/ml-class)), (+,DbfsMountPoint(s3a://prabakar-test, /mnt/prabakar-s3)), (+,DbfsMountPoint(s3a://chris-s3-us-west3, /mnt/chris/mount_test3)), (+,DbfsMountPoint(wasbs://opsstoragedev.blob.core.windows.net, /mnt/blade-inspections)), (+,DbfsMountPoint(s3a://e2billing, /mnt/e2mount)), (+,DbfsMountPoint(s3a://test-bucket, /mnt/$MountName)), (+,DbfsMountPoint(s3a://arjun-assume-role-test-bucket-1/, /mnt/arjun-assume-role-test-bucket-Arjun-Test-2)), (+,DbfsMountPoint(s3a://daya-mount-test, /mnt/daya-mount-test/dayatest)), (+,DbfsMountPoint(s3a://mohit-test-bucket, /mnt/mmtest)), (+,DbfsMountPoint(s3a://rakesh-databricks, /mnt/rakeshparija)), (+,DbfsMountPoint(s3a://rakesh-test-bucket2, /mnt/rakesh-mount-bucket2)), (+,DbfsMountPoint(s3a://nimmanimma, /mnt/nimmanimma2)), (+,DbfsMountPoint(s3a://karthikshivanna/test, /mnt/Nanneghj)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west2, /mnt/empty)), (+,DbfsMountPoint(s3a://cksparkbucket/delta-write/parent1/parent2/parent3/parent4, /mnt/test_child_dir)), (+,DbfsMountPoint(s3a://daya-mount-test, /mnt/daya-mount-test)), (+,DbfsMountPoint(s3a://arjun-assume-role-test-bucket, /mnt/arjun-assume-role-test-bucket-1)), (+,DbfsMountPoint(s3a://dbc-demo-data, /mnt/vgiri)), (+,DbfsMountPoint(s3a://arjun-minio-test, /mnt/arjun-minion-test-4)), (+,DbfsMountPoint(s3a://db-test-db, /mnt/db-test-karan-db)), (+,DbfsMountPoint(s3a://test-bucket-rw/, /mnt/ashwin/read)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovdbz)), (+,DbfsMountPoint(s3a://arjun-assume-role-test, /mnt/arjun-assumerole-test)), (+,DbfsMountPoint(s3a://shashitest/, /mnt/$shashitest2/)), (+,DbfsMountPoint(s3a://abhinavs3test/, /mnt/abhinav/s3Test)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test6)), (+,DbfsMountPoint(s3a://ashishbucketmounted, /mnt/ashishmount)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur_s11)), (+,DbfsMountPoint(s3a://vikas-testing, /mnt/vikas)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/p01)), (+,DbfsMountPoint(s3a://sajeshnewfolder, /mnt/sajeshnewfolder)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovtestmount)), (+,DbfsMountPoint(s3a://drg-snowflake-databricks-lotpoc, /mnt/ravi-drg-mnt)), (+,DbfsMountPoint(s3a://databricks-sajesh/us-east-1, /mnt/sajesh_with_region)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur_mnt21)), (+,DbfsMountPoint(s3a://piyushpocbucket, /mnt/piyushnewmount)), (+,DbfsMountPoint(s3a://karthikshivanna, /mnt/Nane)), (+,DbfsMountPoint(s3a://abhideltatest, /mnt/DeltaAutoLoaderSep)), (+,DbfsMountPoint(s3a://chris-s3-us-west2, /mnt/empty1)), (+,DbfsMountPoint(wasbs://ttprimary.blob.core.windows.net/, /mnt/webinar)), (+,DbfsMountPoint(s3a://chris-test-mount1, /mnt/mount_test/mount_issue1)), (+,DbfsMountPoint(s3a://rakesh-databricks/, /mnt/rakesh)), (+,DbfsMountPoint(s3a://vikasreddya, /mnt/dds3autoloader)), (+,DbfsMountPoint(s3a://ankur-s3, /mnt/ankurs3-1)), (+,DbfsMountPoint(s3a://ornan-deployment, /mnt/ornan-deployment-bucket)), (+,DbfsMountPoint(s3a://prem-audit-log, /mnt/audit)), (+,DbfsMountPoint(s3a://chris-s3-us-west3, /mnt/chris-s3-us-west3)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test2)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test12)), (+,DbfsMountPoint(s3a://gobi-s3-test, /mnt/test-s3-requestor-pays-gobi)), (+,DbfsMountPoint(s3a://ashwin-crossaccount, /mnt/testmount1)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test7)), (+,DbfsMountPoint(s3a://testmmbucket, /mnt/mymountpoint)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur_s1)), (+,DbfsMountPoint(s3a://abitest111/FOO, /mnt/abitest1)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/, /mnt//cust-success-bucket_new)), (+,DbfsMountPoint(s3a://testingabizer, /mnt/testingabizer)), (+,DbfsMountPoint(s3a://abitest1111, /mnt/Test-Encryption)), (+,DbfsMountPoint(s3a://karthik140391/Karthik, /mnt/karthik-shivanna)), (+,DbfsMountPoint(s3a://daya-test-bucket, /mnt/daya-test-bucket123)), (+,DbfsMountPoint(s3a://ankur-test1/, /mnt/$ankurTest1/)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/etlp1s-shanmugavel.chandrakasu@databricks.com-ss)), (+,DbfsMountPoint(s3a://redshifttmpbucket/, /mnt/redshifttmpbucket)), (+,DbfsMountPoint(s3a://rakesh-databricks, /mnt/rakesh/lib)), (+,DbfsMountPoint(s3a://abitest1111/fo, /mnt/abizer/testbucket)), (+,DbfsMountPoint(s3a://chris-s3-us-west2, /mnt/john_test_mnt)), (+,DbfsMountPoint(s3a://arjun-cust-test-bucket, /mnt/arjun-mount-test)), (+,DbfsMountPoint(s3a://arjun-download-test/, /mnt/arjun-download-test1/)), (+,DbfsMountPoint(s3a://ma-test-1, /mnt/ma-tmp)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test2)), (+,DbfsMountPoint(s3a://cust-success-bucket/, /mnt/audits)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nkarpovtestmountz)), (+,DbfsMountPoint(s3a://shashitest, /mnt/shashi1)), (+,DbfsMountPoint(s3a://ankur-ps, /mnt/ankur_mnt22)), (+,DbfsMountPoint(s3a://karthik, /mnt/karthik)), (+,DbfsMountPoint(s3a://abitest1111/fo, /mnt/abizer/testbucket2)), (+,DbfsMountPoint(s3a://arjun-assume-role-test-bucket-1, /mnt/john-assume-role-test-bucket-1-field)), (+,DbfsMountPoint(s3a://cksparkbucket/, /mnt/mount_testing)), (+,DbfsMountPoint(s3a://databricks-sajesh/us-east-1, /mnt/region)), (+,DbfsMountPoint(wasbs://arjunblobtest.blob.core.windows.net, /mnt/tcpdump/)), (+,DbfsMountPoint(s3a://arjun-minio-test, /mnt/arjun-minion-test)), (+,DbfsMountPoint(s3a://ankur-test1/Training_Prog.xlsx, /mnt/$ankurTest/)), (+,DbfsMountPoint(s3a://arjun-cross-account-test-bucket/, /mnt/arjun-cross-account-test-bucket)), (+,DbfsMountPoint(s3a://chris-s3-us-west2, /mnt/chris/mount_test0)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test0001)), (+,DbfsMountPoint(s3a://xin-test-redshift, /mnt/xin)), (+,DbfsMountPoint(s3a://neha-db-test-bucket, /mnt/testneha)), (+,DbfsMountPoint(s3a://abhinavs3test, /mnt/REPLACE_WITH_YOUR_MOUNT_NAME)), (+,DbfsMountPoint(s3a://ankur-test1/, /mnt/$ankur2/)), (+,DbfsMountPoint(s3a://prabakar-test, /mnt/prabakar-mounttest)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test001)), (+,DbfsMountPoint(s3a://nimmanimma, /mnt/nimmanimma1)), (+,DbfsMountPoint(s3a://test-spark-s3, /mnt/ashwin/kms_encrypt)), (+,DbfsMountPoint(s3a://db-vik, /mnt/vikas_bucket)), (+,DbfsMountPoint(s3a://databricks-corp-training/common, /mnt/etlp1s-festus.yeboah@databricks.com-si)), (+,DbfsMountPoint(s3a://rahulspark, /mnt/rahul1)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test3)), (+,DbfsMountPoint(s3a://arjun-cust-success-cluster-logs, /mnt/john-cust-success-clusterlogs)), (+,DbfsMountPoint(s3a://personal-sajesh/, /mnt/sajeshm/)), (+,DbfsMountPoint(s3a://databricks-sajesh, /mnt/sajesh)), (+,DbfsMountPoint(s3a://arjun-field-eng-s3-cross-account-test-4/, /mnt/arjun-field-eng-s3-cross-account-test-4/)), (+,DbfsMountPoint(s3a://databrics-deployment-root/cust-success/0/user/hive/warehouse/internal/, /mnt/internal)), (+,DbfsMountPoint(s3a://prabakar-test, /mnt/pkrmnt1)), (+,DbfsMountPoint(s3a://ravi-cs-rsa-test, /mnt/ravi-cs-test)), (+,DbfsMountPoint(s3a://databricks-us-west-2-billable-usage-prod, /mnt/bill_report)), (+,DbfsMountPoint(s3a://ashwin-crossaccount/, /mnt/ashwin/nike_test)), (+,DbfsMountPoint(s3a://rakesh-databricks, /mnt/rakesh-test-mount)), (+,DbfsMountPoint(s3a://databrics-deployment-root/cust-success/0/user/hive/warehouse/internal/, /mnt/internal2)), (+,DbfsMountPoint(s3a://ankur-test1/, /mnt/ankurTest1/)), (+,DbfsMountPoint(s3a://abhideltatest/, /mnt/annapurna)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test4)), (+,DbfsMountPoint(s3a://cust-success-bucket/cust-success/auditlogs/, /mnt//sajesh-audit-logs_test)), (+,DbfsMountPoint(s3a://ashwin-crossaccount, /mnt/ashwin/cross_account_acl)), (+,DbfsMountPoint(s3a://databricks-sajesh/testSubFolder1, /mnt/sajeshSubFolder)), (+,DbfsMountPoint(s3a://chris-s3-us-west1, /mnt/chris-s3-us-west1)), (+,DbfsMountPoint(s3a://daya-assumerole-testbucket, /mnt/daya-assumerole-test)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test3)), (+,DbfsMountPoint(s3a://e2billing, /mnt/e2mount-defaultkms)), (+,DbfsMountPoint(s3a://abhideltatest, /mnt/abhi_delta_test)), (+,DbfsMountPoint(s3a://iptestdb, /mnt/test_iptestdb_mnt)), (+,DbfsMountPoint(s3a://xin-new-iam-role, /mnt/xin1)), (+,DbfsMountPoint(s3a://prod-l1-datalake-v1-scratchdb, /mnt/test-mount-test)), (+,DbfsMountPoint(s3a://databrics-deployment-root/cust-success/0, /)), (+,DbfsMountPoint(s3a://chris-s3-us-west2/, /mnt/newrelic)), (+,DbfsMountPoint(s3a://test-rakesh-subfolder, /mnt/rakesh-test-subfolder)), (+,DbfsMountPoint(s3a://ashwin-crossaccount/test1, /mnt/ashwin/hp_test)), (+,DbfsMountPoint(s3a://nimmanimma/, /mnt/$rohitnimma/)), (+,DbfsMountPoint(s3a://gauravdbsupport, /mnt/gaurav_mp)), (+,DbfsMountPoint(s3a://ashwin-test-regeneron, /mnt/test/ashwin-test-regeneron)), (+,DbfsMountPoint(s3a://nimmanimma, /mnt/nimmanimma)), (+,DbfsMountPoint(s3a://neha-db-test-bucket, /mnt/neha-test)), (+,DbfsMountPoint(s3a://prabakar-test, /mnt/prabakar-init)), (+,DbfsMountPoint(s3a://nkarpovdb, /mnt/nktest)), (+,DbfsMountPoint(s3a://chrisz-s3-us-west/, /mnt/mount_test/test11)), (+,DbfsMountPoint(s3a://festus-lib-test, /mnt/festus_mount)), (+,DbfsMountPoint(s3a://ganesh-s3-bucket, /mnt/tmp/ganesh-s3-bucket-using-keys)), (+,DbfsMountPoint(s3a://pradeepkumarbucket, /mnt/p02)))
21/04/12 15:14:22 INFO DatabricksFileSystemV2Factory: Creating S3A file system for s3a://databrics-deployment-root
21/04/12 15:14:23 INFO S3AFileSystem: Initializing S3AFileSystem as class shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
21/04/12 15:14:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.205.237.211:48496) with ID 1
21/04/12 15:14:23 INFO S3AFileSystem:V3: S3 configuration compatibility mode is enabled
21/04/12 15:14:23 INFO S3AFileSystem:V3: FS_CONF_BACK_COMPAT [UNSUPPORTED] Ignoring value SessionToken for key fs.s3a.credentialsType as unsupported
21/04/12 15:14:23 INFO S3AFileSystem:V3: FS_CONF_BACK_COMPAT [UPDATE] Both configuration keys fs.s3.buffer.dir and fs.s3a.buffer.dir are set, ignoring update
21/04/12 15:14:23 INFO S3AFileSystem:V3: Initializing S3AFileSystem for databrics-deployment-root
21/04/12 15:14:23 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
21/04/12 15:14:23 INFO MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
21/04/12 15:14:23 INFO MetricsSystemImpl: s3a-file-system metrics system started
21/04/12 15:14:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.205.237.211:39249 with 10.4 GiB RAM, BlockManagerId(1, 10.205.237.211, 39249, None)
21/04/12 15:14:23 WARN ApacheUtils: NoSuchMethodException was thrown when disabling normalizeUri. This indicates you are using an old version (< 4.5.8) of Apache http client. It is recommended to use http client version >= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See https://github.com/aws/aws-sdk-java/issues/1919 for more information
21/04/12 15:14:23 INFO deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key
21/04/12 15:14:24 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
21/04/12 15:14:24 INFO HiveConf: Found configuration file file:/databricks/hive/conf/hive-site.xml
21/04/12 15:14:24 INFO HiveUtils: Initializing execution hive, version 2.3.7
21/04/12 15:14:24 INFO SessionState: Created local directory: /local_disk0/tmp/root
21/04/12 15:14:24 INFO SessionState: Created HDFS directory: /tmp/hive/root/9cb527c8-6cf6-404a-aec7-135629d32102
21/04/12 15:14:25 INFO SessionState: Created local directory: /local_disk0/tmp/root/9cb527c8-6cf6-404a-aec7-135629d32102
21/04/12 15:14:25 INFO SessionState: Created HDFS directory: /tmp/hive/root/9cb527c8-6cf6-404a-aec7-135629d32102/_tmp_space.db
21/04/12 15:14:25 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is /user/hive/warehouse
21/04/12 15:14:25 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
21/04/12 15:14:25 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
21/04/12 15:14:25 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
21/04/12 15:14:25 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
21/04/12 15:14:25 INFO AbstractService: Service:OperationManager is inited.
21/04/12 15:14:25 INFO AbstractService: Service:SessionManager is inited.
21/04/12 15:14:25 INFO AbstractService: Service: CLIService is inited.
21/04/12 15:14:25 INFO AbstractService: Service:ThriftHttpCLIService is inited.
21/04/12 15:14:25 INFO AbstractService: Service: HiveServer2 is inited.
21/04/12 15:14:25 INFO AbstractService: Service:OperationManager is started.
21/04/12 15:14:25 INFO AbstractService: Service:SessionManager is started.
21/04/12 15:14:25 INFO AbstractService: Service:CLIService is started.
21/04/12 15:14:25 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/04/12 15:14:25 INFO ObjectStore: ObjectStore, initialize called
21/04/12 15:14:25 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
21/04/12 15:14:25 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/04/12 15:14:25 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/04/12 15:14:27 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/04/12 15:14:32 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
21/04/12 15:14:32 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/04/12 15:14:32 INFO ObjectStore: Initialized ObjectStore
21/04/12 15:14:33 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
21/04/12 15:14:33 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore root@127.0.1.1
21/04/12 15:14:33 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/04/12 15:14:33 INFO HiveMetaStore: Added admin role in metastore
21/04/12 15:14:33 INFO HiveMetaStore: Added public role in metastore
21/04/12 15:14:33 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/04/12 15:14:33 INFO HiveMetaStore: 0: get_databases: default
21/04/12 15:14:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
21/04/12 15:14:33 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...
21/04/12 15:14:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
21/04/12 15:14:33 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore
21/04/12 15:14:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
21/04/12 15:14:33 INFO AbstractService: Service:ThriftHttpCLIService is started.
21/04/12 15:14:33 INFO AbstractService: Service:HiveServer2 is started.
21/04/12 15:14:33 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
21/04/12 15:14:33 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77d0a492{/sqlserver,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@72bef795{/sqlserver/json,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12ce659{/sqlserver/session,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7212d3df{/sqlserver/session/json,null,AVAILABLE,@Spark}
21/04/12 15:14:33 WARN LibraryUtils$: Library file for validation /databricks/.python-env/packages_to_validate.json does not exist
21/04/12 15:14:33 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/04/12 15:14:33 INFO Server: jetty-9.4.18.v20190429; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_275-b01
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@137a70f1{/StreamingQuery,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54351197{/StreamingQuery/json,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43945480{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d96f413{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5c940e28{/static/sql,null,AVAILABLE,@Spark}
21/04/12 15:14:33 INFO session: DefaultSessionIdManager workerName=node0
21/04/12 15:14:33 INFO session: No SessionScavenger set, using defaults
21/04/12 15:14:33 INFO session: node0 Scavenging every 660000ms
21/04/12 15:14:33 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@34c406a4{/,null,STARTING} has uncovered http methods for path: /*
21/04/12 15:14:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@34c406a4{/,null,AVAILABLE}
21/04/12 15:14:33 INFO SslContextFactory: x509=X509@782bcfa4(1,h=[databrickscloud.com],w=[]) for Server@49042ee4[provider=null,keyStore=file:///databricks/keys/jetty-ssl-driver-keystore.jks,trustStore=null]
21/04/12 15:14:33 INFO AbstractConnector: Started ServerConnector@76d92337{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
21/04/12 15:14:33 INFO Server: Started @28284ms
21/04/12 15:14:33 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
21/04/12 15:14:33 INFO DriverDaemon: Starting driver daemon...
21/04/12 15:14:33 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
21/04/12 15:14:33 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
21/04/12 15:14:33 INFO DriverDaemon$: Attempting to run: 'set up ttyd daemon'
21/04/12 15:14:34 INFO DriverDaemon$: Attempting to run: 'Configuring RStudio daemon'
21/04/12 15:14:34 INFO DriverDaemon$$anon$1: Message out thread ready
21/04/12 15:14:34 INFO Server: jetty-9.4.18.v20190429; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_275-b01
21/04/12 15:14:34 INFO AbstractConnector: Started ServerConnector@b91e024{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
21/04/12 15:14:34 INFO Server: Started @28521ms
21/04/12 15:14:34 INFO DriverDaemon: Driver daemon started.
21/04/12 15:14:36 INFO DriverCorral: Loading the root classloader
21/04/12 15:14:36 INFO DriverCorral: Starting sql repl ReplId-623a6-94e16-555dc-6
21/04/12 15:14:36 INFO DriverCorral: Starting sql repl ReplId-6eba3-a3ad8-daa8f-1
21/04/12 15:14:36 INFO DriverCorral: Starting sql repl ReplId-21b2e-76eda-df54c-3
21/04/12 15:14:36 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/04/12 15:14:36 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/04/12 15:14:36 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/04/12 15:14:36 INFO DriverCorral: Starting sql repl ReplId-25963-96862-d1406-c
21/04/12 15:14:36 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/04/12 15:14:36 INFO SQLDriverWrapper: setupRepl:ReplId-623a6-94e16-555dc-6: finished to load
21/04/12 15:14:36 INFO SQLDriverWrapper: setupRepl:ReplId-21b2e-76eda-df54c-3: finished to load
21/04/12 15:14:36 INFO SQLDriverWrapper: setupRepl:ReplId-6eba3-a3ad8-daa8f-1: finished to load
21/04/12 15:14:36 INFO SQLDriverWrapper: setupRepl:ReplId-25963-96862-d1406-c: finished to load
21/04/12 15:14:36 INFO DriverCorral: Starting sql repl ReplId-f88c9-bb395-81f53
21/04/12 15:14:36 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/04/12 15:14:36 INFO SQLDriverWrapper: setupRepl:ReplId-f88c9-bb395-81f53: finished to load
21/04/12 15:14:36 INFO DriverCorral: Starting r repl ReplId-5f50e-c2531-be9a1-f
21/04/12 15:14:36 WARN RDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/04/12 15:14:36 INFO RDriverLocal: 1. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: object created with for ReplId-5f50e-c2531-be9a1-f.
21/04/12 15:14:36 INFO RDriverLocal: 2. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: initializing ...
21/04/12 15:14:36 INFO RDriverLocal: 3. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: started RBackend thread on port 40455
21/04/12 15:14:36 INFO RDriverLocal: 4. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: waiting for SparkR to be installed ...
21/04/12 15:14:40 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/jsoup/jsoup-1.7.2.jar,,NONE))
21/04/12 15:14:40 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/jsoup/jsoup-1.7.2.jar,,NONE)
21/04/12 15:14:40 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/jars/maven/org/jsoup/jsoup-1.7.2.jar as local file /local_disk0/tmp/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar
21/04/12 15:14:40 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/jsoup/jsoup-1.7.2.jar,,NONE) to local file /local_disk0/tmp/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar 
21/04/12 15:14:40 INFO SparkContext: Added file /local_disk0/tmp/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar at spark://10.205.248.177:39021/files/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar with timestamp 1618240480653
21/04/12 15:14:40 INFO Utils: Copying /local_disk0/tmp/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar to /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar
21/04/12 15:14:40 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar at spark://10.205.248.177:39021/jars/addedFile8427690124730666341jsoup_1_7_2-5e83f.jar with timestamp 1618240480693
21/04/12 15:14:40 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/jsoup/jsoup-1.7.2.jar to Spark
21/04/12 15:14:40 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/jsoup/jsoup-1.7.2.jar
21/04/12 15:14:40 INFO DriverCorral: AttachLibraries - candidate libraries: List(PythonPyPiPkgId(simplejson,None,None,List()), PythonPyPiPkgId(pandas,None,None,List()), JavaJarId(dbfs:/mnt/libraries/library.jar,,NONE), PythonEggId(dbfs:/mnt/libraries/library.egg,,NONE), PythonWhlId(dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl,,NONE), PythonWhlId(dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip,,NONE), PythonPyPiPkgId(simplejson,None,Some(https://my-pypi-mirror.com),List()), PythonPyPiPkgId(six,None,Some(https://my-pypi-mirror.com),List()), JavaJarId(dbfs:/FileStore/jars/87871a39_70b6_4803_a95a_586ae0cb12a4-hudi_hadoop_mr_0_5_3-95968.jar,,NONE))
21/04/12 15:14:40 INFO SharedDriverContext: create pypi file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi for project simplejson
21/04/12 15:14:40 INFO SharedDriverContext: Successfully saved library PythonPyPiPkgId(simplejson,None,None,List()) to local file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi 
21/04/12 15:14:40 INFO SparkContext: Added file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi at spark://10.205.248.177:39021/files/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi with timestamp 1618240480865
21/04/12 15:14:40 INFO Utils: Copying /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi to /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi
21/04/12 15:14:40 INFO Utils: Extracting pypi install information from: /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi
21/04/12 15:14:40 INFO Utils: resolved command to be run: List(/databricks/python/bin/pip, install, simplejson, --disable-pip-version-check)
21/04/12 15:14:42 INFO SparkContext: Added JAR /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi at spark://10.205.248.177:39021/jars/__clusterWide_lib_00000_70f21d11a276f42caa5a4f0c06ffe0d7.pypi with timestamp 1618240482331
21/04/12 15:14:42 INFO SharedDriverContext: Successfully attached library simplejson to Spark
21/04/12 15:14:42 INFO LibraryState: Successfully attached library simplejson
21/04/12 15:14:42 INFO SharedDriverContext: create pypi file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi for project pandas
21/04/12 15:14:42 INFO SharedDriverContext: Successfully saved library PythonPyPiPkgId(pandas,None,None,List()) to local file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi 
21/04/12 15:14:42 INFO SparkContext: Added file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi at spark://10.205.248.177:39021/files/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi with timestamp 1618240482352
21/04/12 15:14:42 INFO Utils: Copying /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi to /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi
21/04/12 15:14:42 INFO Utils: Extracting pypi install information from: /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi
21/04/12 15:14:42 INFO Utils: resolved command to be run: List(/databricks/python/bin/pip, install, pandas, --disable-pip-version-check)
21/04/12 15:14:43 INFO SparkContext: Added JAR /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi at spark://10.205.248.177:39021/jars/__clusterWide_lib_00001_3a43b4f88325d94022c0efa9c2fa2f5a.pypi with timestamp 1618240483381
21/04/12 15:14:43 INFO SharedDriverContext: Successfully attached library pandas to Spark
21/04/12 15:14:43 INFO LibraryState: Successfully attached library pandas
21/04/12 15:14:43 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/mnt/libraries/library.jar,,NONE)
21/04/12 15:14:43 ERROR LibraryDownloadManager: Could not download JavaJarId(dbfs:/mnt/libraries/library.jar,,NONE)
java.io.FileNotFoundException: dbfs:/mnt/libraries/library.jar
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:223)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
21/04/12 15:14:43 INFO SharedDriverContext: Failed to attach library dbfs:/mnt/libraries/library.jar to Spark
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.jar
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:223)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.jar
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 85 more
21/04/12 15:14:43 WARN LibraryState: Failed to install library dbfs:/mnt/libraries/library.jar
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.jar
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:223)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.jar
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 85 more
21/04/12 15:14:43 INFO LibraryDownloadManager: Downloading a library that was not in the cache: PythonEggId(dbfs:/mnt/libraries/library.egg,,NONE)
21/04/12 15:14:43 ERROR LibraryDownloadManager: Could not download PythonEggId(dbfs:/mnt/libraries/library.egg,,NONE)
java.io.FileNotFoundException: dbfs:/mnt/libraries/library.egg
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:223)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
21/04/12 15:14:43 INFO SharedDriverContext: Failed to attach library dbfs:/mnt/libraries/library.egg to Spark
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.egg
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:223)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.egg
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 85 more
21/04/12 15:14:43 WARN LibraryState: Failed to install library dbfs:/mnt/libraries/library.egg
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.egg
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:223)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/library.egg
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 85 more
21/04/12 15:14:43 INFO LibraryDownloadManager: Downloading a library that was not in the cache: PythonWhlId(dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl,,NONE)
21/04/12 15:14:43 ERROR LibraryDownloadManager: Could not download PythonWhlId(dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl,,NONE)
java.io.FileNotFoundException: dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWideWhlFile(SharedDriverContext.scala:238)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:221)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
21/04/12 15:14:43 INFO SharedDriverContext: Failed to attach library dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl to Spark
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWideWhlFile(SharedDriverContext.scala:238)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:221)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 86 more
21/04/12 15:14:43 WARN LibraryState: Failed to install library dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWideWhlFile(SharedDriverContext.scala:238)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:221)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/mlflow-0.0.1.dev0-py2-none-any.whl
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 86 more
21/04/12 15:14:43 INFO LibraryDownloadManager: Downloading a library that was not in the cache: PythonWhlId(dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip,,NONE)
21/04/12 15:14:43 ERROR LibraryDownloadManager: Could not download PythonWhlId(dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip,,NONE)
java.io.FileNotFoundException: dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWideWhlFile(SharedDriverContext.scala:238)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:221)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
21/04/12 15:14:43 INFO SharedDriverContext: Failed to attach library dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip to Spark
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWideWhlFile(SharedDriverContext.scala:238)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:221)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 86 more
21/04/12 15:14:43 WARN LibraryState: Failed to install library dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.downloadLibrary(LibraryDownloadManager.scala:100)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWideWhlFile(SharedDriverContext.scala:238)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:221)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: dbfs:/mnt/libraries/wheel-libraries.wheelhouse.zip
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:464)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager.com$databricks$backend$daemon$driver$LibraryDownloadManager$$fetchLibrary0(LibraryDownloadManager.scala:151)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:41)
	at com.databricks.backend.daemon.driver.LibraryDownloadManager$$anon$1.load(LibraryDownloadManager.scala:36)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 86 more
21/04/12 15:14:43 INFO SharedDriverContext: Failed to attach library simplejson to Spark
org.apache.spark.SparkException: PyPI package simplejson has been installed already. The previously installed package is `simplejson`. Please remove the duplicate PyPI package and restart the cluster.
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWidePyPIPkgFile(SharedDriverContext.scala:285)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:225)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
21/04/12 15:14:43 WARN LibraryState: Failed to install library simplejson
org.apache.spark.SparkException: PyPI package simplejson has been installed already. The previously installed package is `simplejson`. Please remove the duplicate PyPI package and restart the cluster.
	at com.databricks.backend.daemon.driver.SharedDriverContext.getClusterWidePyPIPkgFile(SharedDriverContext.scala:285)
	at com.databricks.backend.daemon.driver.SharedDriverContext.getLibraryLocalFile(SharedDriverContext.scala:225)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$2(SharedDriverContext.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:71)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:71)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:174)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:167)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibrariesToSpark(DriverCorral.scala:748)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:710)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:889)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:885)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:53)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:80)
	at com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:49)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:16)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:16)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:48)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:637)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:637)
	at com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:560)
	at com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:327)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:156)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:156)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:315)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:205)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:505)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
21/04/12 15:14:43 INFO SharedDriverContext: create pypi file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi for project six
21/04/12 15:14:43 INFO SharedDriverContext: Successfully saved library PythonPyPiPkgId(six,None,Some(https://my-pypi-mirror.com),List()) to local file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi 
21/04/12 15:14:43 INFO SparkContext: Added file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi at spark://10.205.248.177:39021/files/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi with timestamp 1618240483704
21/04/12 15:14:43 INFO Utils: Copying /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi to /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi
21/04/12 15:14:43 INFO Utils: Extracting pypi install information from: /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi
21/04/12 15:14:43 INFO Utils: resolved command to be run: List(/databricks/python/bin/pip, install, six, --index-url, https://my-pypi-mirror.com, --disable-pip-version-check)
21/04/12 15:14:44 INFO SparkContext: Added JAR /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi at spark://10.205.248.177:39021/jars/__clusterWide_lib_00002_f52b5e449a2303c031a0c3a1109360bf.pypi with timestamp 1618240484617
21/04/12 15:14:44 INFO SharedDriverContext: Successfully attached library six to Spark
21/04/12 15:14:44 INFO LibraryState: Successfully attached library six
21/04/12 15:14:44 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/87871a39_70b6_4803_a95a_586ae0cb12a4-hudi_hadoop_mr_0_5_3-95968.jar,,NONE)
21/04/12 15:14:44 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/jars/87871a39_70b6_4803_a95a_586ae0cb12a4-hudi_hadoop_mr_0_5_3-95968.jar as local file /local_disk0/tmp/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar
21/04/12 15:14:44 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/87871a39_70b6_4803_a95a_586ae0cb12a4-hudi_hadoop_mr_0_5_3-95968.jar,,NONE) to local file /local_disk0/tmp/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar 
21/04/12 15:14:44 INFO SparkContext: Added file /local_disk0/tmp/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar at spark://10.205.248.177:39021/files/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar with timestamp 1618240484733
21/04/12 15:14:44 INFO Utils: Copying /local_disk0/tmp/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar to /local_disk0/spark-10749251-8231-4d98-b20d-0a5fd268267e/userFiles-fd6ca47f-ee66-4e60-b093-79981fd82ced/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar
21/04/12 15:14:44 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar at spark://10.205.248.177:39021/jars/addedFile832662029358698751587871a39_70b6_4803_a95a_586ae0cb12a4_hudi_hadoop_mr_0_5_3_95968-6a639.jar with timestamp 1618240484744
21/04/12 15:14:44 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/87871a39_70b6_4803_a95a_586ae0cb12a4-hudi_hadoop_mr_0_5_3-95968.jar to Spark
21/04/12 15:14:44 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/87871a39_70b6_4803_a95a_586ae0cb12a4-hudi_hadoop_mr_0_5_3-95968.jar
21/04/12 15:14:54 INFO RDriverLocal$: SparkR installation completed.
21/04/12 15:14:54 INFO RDriverLocal: 5. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: launching R process ...
21/04/12 15:14:54 INFO RDriverLocal: 6. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: cgroup isolation disabled, not placing R process in REPL cgroup.
21/04/12 15:14:54 INFO RDriverLocal: 7. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: starting R process on port 1100 (attempt 1) ...
21/04/12 15:14:54 INFO RDriverLocal: 8. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: setting up BufferedStreamThread with bufferSize: 100.
21/04/12 15:14:55 INFO RDriverLocal: 9. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: R process started with RServe listening on port 1100.
21/04/12 15:14:56 INFO RDriverLocal: 10. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: starting interpreter to talk to R process ...
21/04/12 15:14:57 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
21/04/12 15:14:57 INFO RDriverLocal: 11. RDriverLocal.5f7d43cb-d394-4c09-8ab8-967b02bc48f5: R interpretter is connected.
21/04/12 15:14:57 INFO RDriverWrapper: setupRepl:ReplId-5f50e-c2531-be9a1-f: finished to load
21/04/12 15:19:11 INFO DriverCorral: DBFS health check ok
21/04/12 15:19:11 WARN MetastoreMonitor: Failed to connect to the metastore InternalMysqlMetastore(DbMetastoreConfig{host=cust-success-recovered-from-snapshot.caj77bnxuhme.us-west-2.rds.amazonaws.com, port=3306, dbName=organization0, user=[REDACTED]}). (timeSinceLastSuccess=300003)
java.lang.IllegalArgumentException: A health check named database already exists
	at com.codahale.metrics.health.HealthCheckRegistry.register(HealthCheckRegistry.java:101)
	at com.databricks.instrumentation.Instrumented$Dsl.instrumentJdbi(Instrumented.scala:242)
	at com.databricks.common.database.DatabaseUtils$.createDBI(DatabaseUtils.scala:170)
	at com.databricks.common.database.DatabaseUtils$.withDBI(DatabaseUtils.scala:497)
	at com.databricks.backend.daemon.driver.MetastoreMonitor.checkMetastore(MetastoreMonitor.scala:177)
	at com.databricks.backend.daemon.driver.MetastoreMonitor.$anonfun$doMonitor$1(MetastoreMonitor.scala:154)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:432)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:94)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:94)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:413)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:94)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$2(NamedTimer.scala:103)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:94)
	at com.databricks.logging.UsageLogging.disableTracing(UsageLogging.scala:833)
	at com.databricks.logging.UsageLogging.disableTracing$(UsageLogging.scala:832)
	at com.databricks.threading.NamedTimer$$anon$1.disableTracing(NamedTimer.scala:94)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$1(NamedTimer.scala:102)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:100)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:101)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
21/04/12 15:19:33 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--gcs-private--protobuf-java-util_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-library_2.12--org.scala-lang__scala-library__2.12.10.jar:file:/databricks/hive/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.11.2_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.yetus--audience-annotations--org.apache.yetus__audience-annotations__0.5.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.hibernate.validator--hibernate-validator--org.hibernate.validator__hibernate-validator__6.1.0.Final.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.4.18.v20190429.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:file:/databricks/hive/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.30.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:file:/databricks/hive/third_party--gcs-private--failureaccess_shaded.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.0_2.12_deploy.jar_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.51.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.4.18.v20190429.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:file:/databricks/hive/common--tracing--tracing-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:file:/databricks/hive/common--client--client-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--commons-logging_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/extern--acl--auth--auth-spark_3.0_2.12_deploy.jar:file:/databricks/hive/common--reflection--reflection-spark_3.0_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:file:/databricks/hive/third_party--gcs-private--guava_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.0.0_shaded.jar:file:/databricks/hive/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.31.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/third_party--gcs-private--proto-google-iam-v1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--gcs-private--j2objc-annotations_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/third_party--gcs-private--flogger_shaded.jar:file:/databricks/hive/third_party--gcs-private--util-hadoop_shaded.jar:file:/databricks/hive/s3commit--common--common-spark_3.0_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-xml_2.12--org.scala-lang.modules__scala-xml_2.12__1.2.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/third_party--gcs-private--grpc-api_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.19.0_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-extensions_shaded.jar:file:/databricks/hive/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.0_2.12_deploy_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.scalactic--scalactic_2.12--org.scalactic__scalactic_2.12__3.0.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--jetty-client--jetty-util_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-netty-shaded_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave_io.zipkin.brave__brave__5.12.4_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-reflect_2.12--org.scala-lang__scala-reflect__2.12.10.jar:file:/databricks/hive/third_party--gcs-private--grpc-core_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_jakarta.annotation_jakarta.annotation-api_jakarta.annotation__jakarta.annotation-api__1.3.5_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--grpc-alts_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:file:/databricks/hive/third_party--gcs-private--auto-value-annotations_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.9_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.8_container_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-client_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/databricks/hive/third_party--gcs-private--proto-google-common-protos_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--gcs-private--grpc-stub_shaded.jar:file:/databricks/hive/common--util--locks-spark_3.0_2.12_deploy.jar:file:/databricks/hive/jsonutil--jsonutil-spark_3.0_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/third_party--gcs-private--annotations_shaded.jar:file:/databricks/hive/extern--extern-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.31.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/databricks/hive/third_party--datalake--datalake-spark_3.0_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.3.4_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.zipkin.zipkin2_zipkin_io.zipkin.zipkin2__zipkin__2.21.1_shaded.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--azure--com.microsoft.rest__client-runtime__1.7.8_container_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:file:/databricks/hive/third_party--gcs-private--google-api-client-java6_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.51.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:file:/databricks/hive/third_party--gcs-private--flogger-system-backend_shaded.jar:file:/databricks/hive/third_party--gcs-private--gcsio_shaded.jar:file:/databricks/hive/common--hadoop--hadoop-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.31.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/third_party--gcs-private--google-api-client-jackson2_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-grpclb_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/third_party--gcs-private--google-http-client_shaded.jar:file:/databricks/hive/third_party--gcs-private--gcs-shaded-spark_3.0_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter-brave_io.zipkin.reporter2__zipkin-reporter-brave__2.15.0_shaded.jar:file:/databricks/hive/third_party--gcs-private--gson_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/third_party--gcs-private--javax.annotation-api_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:file:/databricks/hive/third_party--gcs-private--google-api-services-storage_shaded.jar:file:/databricks/hive/third_party--gcs-private--jsr305_shaded.jar:file:/databricks/hive/third_party--gcs-private--util_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:file:/databricks/hive/third_party--gcs-private--checker-qual_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:file:/databricks/hive/third_party--gcs-private--error_prone_annotations_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-epoll-linux-x86_64_io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-tcnative-boringssl-static_io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--gcs-private--google-oauth-client-java6_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.11.2_shaded.jar:file:/databricks/hive/third_party--gcs-private--api-common_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-api-client_shaded.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/third_party--jetty-client--jetty-http_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/databricks/hive/common--path--path-spark_3.0_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.4.18.v20190429.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.12.0_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.31.1_shaded.jar:file:/databricks/hive/third_party--gcs-private--protobuf-java_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:file:/databricks/hive/third_party--gcs-private--commons-lang3_shaded.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.18_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-io_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.4.18.v20190429.jar:file:/databricks/hive/third_party--gcs-private--gcs-connector_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/third_party--gcs-private--google-auth-library-credentials_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/third_party--gcs-private--listenablefuture_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.4.18.v20190429.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--perfmark-api_shaded.jar:file:/databricks/hive/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.31.1_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.31.1_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-auth_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/databricks/hive/common--jetty--client--client-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.5.4_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.4.18.v20190429.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:file:/databricks/hive/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.4.18.v20190429.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-parser-combinators_2.12--org.scala-lang.modules__scala-parser-combinators_2.12__1.1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/api-base--api-base-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/third_party--gcs-private--google-http-client-jackson2_shaded.jar:file:/databricks/hive/third_party--gcs-private--commons-codec_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.1.0_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.47.Final.jar:file:/databricks/hive/third_party--gcs-private--opencensus-api_shaded.jar:file:/databricks/hive/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave-instrumentation-http_io.zipkin.brave__brave-instrumentation-http__5.12.4_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__1.17.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.18.v20190429.jar:file:/databricks/hive/s3--s3-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.0_2.12_deploy_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter_io.zipkin.reporter2__zipkin-reporter__2.15.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.3.2.Final.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.4.18.v20190429.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.30.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.18.v20190429.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/s3commit--client--client-spark_3.0_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-context_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/third_party--gcs-private--httpcore_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/databricks/hive/third_party--gcs-private--httpclient_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__29.0-android_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-oauth-client_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.51.Final_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.31.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-protobuf_shaded.jar:file:/databricks/hive/dbfs--exceptions--exceptions-spark_3.0_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:file:/databricks/hive/common--lazy--lazy-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common_io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:file:/databricks/hive/api-base--api-base_java-spark_3.0_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.0_2.12_deploy_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/third_party--gcs-private--jackson-core_shaded.jar:file:/databricks/hive/third_party--gcs-private--checker-compat-qual_shaded.jar:file:/databricks/hive/daemon--data--client--client-spark_3.0_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/databricks/hive/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-brave_com.linecorp.armeria__armeria-brave__1.0.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/common--credentials--credentials-spark_3.0_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.14.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.11.2_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.5.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/----workspace_spark_3_0--vendor--spark-ganglia-lgpl--libmetrics-ganglia.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.0.jar:file:/databricks/hive/bonecp-configs.jar
21/04/12 15:19:34 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21/04/12 15:19:34 INFO ObjectStore: ObjectStore, initialize called
21/04/12 15:19:34 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/04/12 15:19:34 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/04/12 15:19:34 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/04/12 15:19:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/04/12 15:19:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/04/12 15:19:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/04/12 15:19:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/04/12 15:19:36 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21/04/12 15:19:36 INFO ObjectStore: Initialized ObjectStore
21/04/12 15:19:36 INFO HiveMetaStore: Added admin role in metastore
21/04/12 15:19:36 INFO HiveMetaStore: Added public role in metastore
21/04/12 15:19:36 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/04/12 15:19:36 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
21/04/12 15:19:36 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is /user/hive/warehouse
21/04/12 15:19:36 INFO HiveMetaStore: 0: get_database: default
21/04/12 15:19:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21/04/12 15:19:36 INFO HiveMetaStore: 0: get_database: default
21/04/12 15:19:36 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21/04/12 15:19:36 INFO DriverCorral: Metastore health check ok
